{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP HW 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nikhilkoditala/CSC-8980-NLP/blob/main/NLP_HW_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-hLkNkb1CiG"
      },
      "source": [
        "# Name: Nikhil Koditala\n",
        "\n",
        "# Panther ID: 002571023"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yu96XLSldu3X"
      },
      "source": [
        "1. Create a cosine similarity matrix for all of Shakespeare’s works found in the provided file. This will result in a 42 by 42 matrix with the cosine similarity between each of his works. In other words, calculate the document-wise cosine similarity between all of Shakepeare’s works. (50 points). ​Use TF_IDF for this. Note, you can use the Cosine Similarity function on scikit-learn or implement your own, but no other library/package is allowed.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujukGFzGdpkm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6de3b334-0adf-4f5f-d9c4-baa0db39c95c"
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics.pairwise import linear_kernel\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import os, random\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('reuters')\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BuMi-HaogZG",
        "outputId": "1ab83da9-b9a5-44d6-e230-a3db79400116"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OP9UwHywhIGQ"
      },
      "source": [
        "folder_path = '/content/drive/MyDrive/shakespeares-works_TXT_FolgerShakespeare'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MxMIctajkAK"
      },
      "source": [
        "txt_files = os.listdir(folder_path)\n",
        "data = []\n",
        "for file in txt_files:\n",
        "   data.append(open(os.path.join(folder_path,file), 'r').read())"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjtYT4sij9hP"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(data)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbRO0TUQkxAn",
        "outputId": "4028d91d-0f84-477c-e7c8-74e16a6152f9"
      },
      "source": [
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "print(cosine_sim)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.90761656 0.77494139 ... 0.81097319 0.59536997 0.66267568]\n",
            " [0.90761656 1.         0.78910615 ... 0.83414428 0.6086646  0.67148272]\n",
            " [0.77494139 0.78910615 1.         ... 0.83172303 0.59251467 0.65190502]\n",
            " ...\n",
            " [0.81097319 0.83414428 0.83172303 ... 1.         0.61221239 0.67887912]\n",
            " [0.59536997 0.6086646  0.59251467 ... 0.61221239 1.         0.52581905]\n",
            " [0.66267568 0.67148272 0.65190502 ... 0.67887912 0.52581905 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xeZ_dgmEk4a5"
      },
      "source": [
        "2 Write a function that takes the previous matrix and a number ​n​ as parameters (nothing else will be accepted) and return the top ​n​ similar works. Use the function to output the top 10 similar works. (30 points)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BHiik_iCk1B8"
      },
      "source": [
        "def n_similar_works(cosine_sim, N):\n",
        "  for i in range(len(cosine_sim)):\n",
        "    cosine_sim[i][i] = 0 # Making diagonal elements as zero, to discard them from sorting algorithm\n",
        "\n",
        "  index_1d = cosine_sim.flatten().argsort()[-(2*N):]\n",
        "  x_idx, y_idx = np.unravel_index(index_1d, cosine_sim.shape)\n",
        "  \n",
        "  x_idx = np.flip(x_idx)\n",
        "  y_idx = np.flip(y_idx)\n",
        "\n",
        "  similar_works = []\n",
        "  for i in range(0,len(x_idx),2):\n",
        "    x = x_idx[i]\n",
        "    y = y_idx[i]\n",
        "    if(x != y):\n",
        "      similar_works.append((x,y,cosine_sim[x][y]))\n",
        "  \n",
        "  return similar_works"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2fIp5HXAR96r"
      },
      "source": [
        "similar_works = n_similar_works(cosine_sim,10)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uRj0VoixoiP2",
        "outputId": "f5144508-2d76-4141-b5fc-bef9524a56ff"
      },
      "source": [
        "print('Similar works are:')\n",
        "for work in similar_works:\n",
        "  print(txt_files[work[0]],' and ',txt_files[work[1]],' - ',work[2])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Similar works are:\n",
            "venus-and-adonis_TXT_FolgerShakespeare.txt  and  lucrece_TXT_FolgerShakespeare.txt  -  0.9175430138884013\n",
            "henry-iv-part-2_TXT_FolgerShakespeare.txt  and  henry-iv-part-1_TXT_FolgerShakespeare.txt  -  0.9076165568984116\n",
            "shakespeares-sonnets_TXT_FolgerShakespeare.txt  and  lucrece_TXT_FolgerShakespeare.txt  -  0.867768874887524\n",
            "henry-vi-part-2_TXT_FolgerShakespeare.txt  and  henry-vi-part-1_TXT_FolgerShakespeare.txt  -  0.84379159802984\n",
            "richard-ii_TXT_FolgerShakespeare.txt  and  richard-iii_TXT_FolgerShakespeare.txt  -  0.838952501010752\n",
            "shakespeares-sonnets_TXT_FolgerShakespeare.txt  and  venus-and-adonis_TXT_FolgerShakespeare.txt  -  0.8381452108269781\n",
            "henry-viii_TXT_FolgerShakespeare.txt  and  henry-v_TXT_FolgerShakespeare.txt  -  0.8363042342905281\n",
            "henry-vi-part-2_TXT_FolgerShakespeare.txt  and  henry-vi-part-3_TXT_FolgerShakespeare.txt  -  0.8354926670186525\n",
            "richard-ii_TXT_FolgerShakespeare.txt  and  henry-vi-part-2_TXT_FolgerShakespeare.txt  -  0.834350777065286\n",
            "king-john_TXT_FolgerShakespeare.txt  and  henry-v_TXT_FolgerShakespeare.txt  -  0.8342954531999515\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0kUWlpom0e8"
      },
      "source": [
        "\n",
        "3 Using the code from the Language Models II class, train two simple language models using all of the files (together) in shakespeares-works_TXT_FolgerShakespeare.zip. One model should be trained using bigrams, the other using trigrams. (40 points)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xSqQwF0ycAbf"
      },
      "source": [
        "tokens = []\n",
        "for doc in data:\n",
        "  tokens.append(nltk.word_tokenize(doc))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdRtbYCpc3E7"
      },
      "source": [
        "a) Bigrams Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "POpN3YOnm0zV"
      },
      "source": [
        "from nltk.corpus import reuters\n",
        "from nltk import bigrams, trigrams\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "# Create a placeholder for model\n",
        "model_bigram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurence  \n",
        "for sentence in tokens:\n",
        "    for w1, w2 in bigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model_bigram[w1][w2] += 1\n",
        "\n",
        "# Let's transform the counts to probabilities\n",
        "for w1 in model_bigram:\n",
        "    total_count = float(sum(model_bigram[w1].values()))\n",
        "    for w2 in model_bigram[w1]:\n",
        "        model_bigram[w1][w2] /= total_count"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4XndjxUinQz",
        "outputId": "67af20d3-d715-4e69-ca5a-435368bd3ab5"
      },
      "source": [
        "dict(model_bigram[\"Trojan\"])"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'!': 0.05128205128205128,\n",
              " \"'s\": 0.02564102564102564,\n",
              " ',': 0.1282051282051282,\n",
              " '.': 0.07692307692307693,\n",
              " ';': 0.02564102564102564,\n",
              " '?': 0.02564102564102564,\n",
              " 'And': 0.02564102564102564,\n",
              " 'and': 0.05128205128205128,\n",
              " 'ass': 0.02564102564102564,\n",
              " 'bleeds': 0.02564102564102564,\n",
              " 'blood': 0.05128205128205128,\n",
              " 'dead': 0.02564102564102564,\n",
              " 'did': 0.02564102564102564,\n",
              " 'drab': 0.02564102564102564,\n",
              " 'hath': 0.02564102564102564,\n",
              " 'horse': 0.02564102564102564,\n",
              " 'is': 0.02564102564102564,\n",
              " 'leaders': 0.02564102564102564,\n",
              " 'lords': 0.02564102564102564,\n",
              " 'mothers': 0.02564102564102564,\n",
              " 'part': 0.02564102564102564,\n",
              " 'prisoner': 0.02564102564102564,\n",
              " 'scorns': 0.02564102564102564,\n",
              " 'so': 0.02564102564102564,\n",
              " 'soldiers': 0.02564102564102564,\n",
              " 'that': 0.02564102564102564,\n",
              " 'trail': 0.02564102564102564,\n",
              " 'trumpets': 0.02564102564102564,\n",
              " 'under': 0.02564102564102564,\n",
              " 'walls': 0.02564102564102564}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-J4amaJdJsS"
      },
      "source": [
        "b) Trigrams Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DInUqWv1dJUx"
      },
      "source": [
        "# Create a placeholder for model\n",
        "model_trigram = defaultdict(lambda: defaultdict(lambda: 0))\n",
        "\n",
        "# Count frequency of co-occurence  \n",
        "for sentence in tokens:\n",
        "    for w1, w2, w3 in trigrams(sentence, pad_right=True, pad_left=True):\n",
        "        model_trigram[(w1, w2)][w3] += 1\n",
        " \n",
        "# Let's transform the counts to probabilities\n",
        "for w1_w2 in model_trigram:\n",
        "    total_count = float(sum(model_trigram[w1_w2].values()))\n",
        "    for w3 in model_trigram[w1_w2]:\n",
        "        model_trigram[w1_w2][w3] /= total_count"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htrE55d8dEHq",
        "outputId": "48a3b430-625e-42d6-fae8-52241aaa2d2e"
      },
      "source": [
        "dict(model_trigram[\"cruel\",\"battle\"])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'here': 1.0}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhLgchQ9m1W5"
      },
      "source": [
        " 4 Write a function that takes the following three parameters: model, list of start words, number of sentences to generate. This function should return the sentences generated as a list. DO NOT print anything to the screen from within the function. Use this function to generate 10 sentences with the bigram model from the previous question, and 5 sentences with the trigram model from the previous question. (30 points)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJ1W1osLm3sV"
      },
      "source": [
        "def gen_sentences(model,start_words,num_sentences):\n",
        "  sentences = []\n",
        "  \n",
        "  for key in model.keys():\n",
        "    break\n",
        "  \n",
        "  if(type(key) == tuple):\n",
        "    for i in range(num_sentences):\n",
        "      text = start_words\n",
        "      sentence_finished = False\n",
        "\n",
        "      while not sentence_finished:\n",
        "        # select a random probability threshold  \n",
        "        r = random.random()\n",
        "        accumulator = .0\n",
        "\n",
        "        for word in model[tuple(text[-2:])].keys():\n",
        "            accumulator += model[tuple(text[-2:])][word]\n",
        "            # select words that are above the probability threshold\n",
        "            if accumulator >= r:\n",
        "                text.append(word)\n",
        "                break\n",
        "\n",
        "        if text[-2:] == [None, None]:\n",
        "            sentence_finished = True\n",
        "        \n",
        "      sentences.append(' '.join([t for t in text if t]))\n",
        "  else:\n",
        "    for i in range(num_sentences):\n",
        "      text = start_words\n",
        "      sentence_finished = False\n",
        "\n",
        "      while not sentence_finished:\n",
        "        # select a random probability threshold  \n",
        "        r = random.random()\n",
        "        accumulator = .0\n",
        "\n",
        "        for word in model[text[-1]].keys():\n",
        "            accumulator += model[text[-1]][word]\n",
        "            # select words that are above the probability threshold\n",
        "            if accumulator >= r:\n",
        "                text.append(word)\n",
        "                break\n",
        "\n",
        "        if text[-1:] == [None]:\n",
        "            sentence_finished = True\n",
        "        \n",
        "      sentences.append(' '.join([t for t in text if t]))\n",
        "\n",
        "  return sentences"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58Vv645Qjb3k"
      },
      "source": [
        "sentences_bigram = gen_sentences(model_bigram,['cruel'],10)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cug-hAo7-v1S"
      },
      "source": [
        "sentences_trigram = gen_sentences(model_trigram,['cruel','battle'],5)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DiI5hRxum_eS"
      },
      "source": [
        "Bonus (20 points): Using the same methodology from questions 1 and 2, create a similarity matrix between the 20 newsgroups corpus. And find the top 5 similar newsgroups."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q56YFn3Xm_zG"
      },
      "source": [
        "twenty_data = fetch_20newsgroups()"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BS7_4DylnBCd"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(twenty_data.data)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83DBQX_ZnHrG",
        "outputId": "c3de780f-f181-4cc7-e5cb-7b3d16f2fd4a"
      },
      "source": [
        "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "print(cosine_sim)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.         0.04405974 0.11017033 ... 0.04433678 0.04457107 0.0329325 ]\n",
            " [0.04405974 1.         0.06242113 ... 0.07373268 0.06959306 0.02439956]\n",
            " [0.11017033 0.06242113 1.         ... 0.07569182 0.06214891 0.02357985]\n",
            " ...\n",
            " [0.04433678 0.07373268 0.07569182 ... 1.         0.02909961 0.00716986]\n",
            " [0.04457107 0.06959306 0.06214891 ... 0.02909961 1.         0.02428174]\n",
            " [0.0329325  0.02439956 0.02357985 ... 0.00716986 0.02428174 1.        ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXm9BYkgnb5g"
      },
      "source": [
        "similar_works = n_similar_works(cosine_sim,5)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8WSZKNrFpBLK",
        "outputId": "36713d7f-8679-4e5b-f813-60237c68e1dd"
      },
      "source": [
        "print(similar_works)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[(10777, 2002, 1.0000000000000002), (5392, 14, 1.0000000000000002), (9989, 800, 1.0), (4495, 4772, 0.9997809868890166), (8665, 4772, 0.9996809512865921)]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}